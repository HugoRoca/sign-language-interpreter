# Sign Language Interpreter

Un intÃ©rprete de lenguaje de seÃ±as (ASL - American Sign Language) implementado en Python usando deep learning. Este proyecto utiliza una arquitectura CNN (Convolutional Neural Network) para reconocer y clasificar seÃ±as del alfabeto ASL.

## ğŸ—ï¸ Arquitectura

El proyecto sigue los principios SOLID y una arquitectura en capas:

```
src/
â”œâ”€â”€ domain/              # Capa de dominio (lÃ³gica de negocio)
â”‚   â”œâ”€â”€ entities/        # Entidades del dominio
â”‚   â”œâ”€â”€ repositories/    # Interfaces de repositorios (abstracciones)
â”‚   â””â”€â”€ services/        # Servicios del dominio
â”œâ”€â”€ infrastructure/      # Capa de infraestructura (implementaciones)
â”‚   â”œâ”€â”€ camera/          # ImplementaciÃ³n de cÃ¡mara (OpenCV)
â”‚   â”œâ”€â”€ data_loaders/    # Carga de datos desde sistema de archivos
â”‚   â”œâ”€â”€ models/          # ImplementaciÃ³n del modelo CNN
â”‚   â”œâ”€â”€ preprocessing/   # Preprocesamiento de imÃ¡genes
â”‚   â””â”€â”€ services/        # Servicios de infraestructura (formaciÃ³n de palabras)
â”œâ”€â”€ application/         # Capa de aplicaciÃ³n (casos de uso)
â”‚   â””â”€â”€ services/        # Servicios de aplicaciÃ³n (entrenamiento, predicciÃ³n, cÃ¡mara)
â””â”€â”€ interfaces/          # Capa de interfaces
    â”œâ”€â”€ cli/             # Interfaz de lÃ­nea de comandos
    â””â”€â”€ camera/          # Interfaz de cÃ¡mara (alternativa)
```

### Principios SOLID aplicados:

- **Single Responsibility**: Cada clase tiene una Ãºnica responsabilidad
- **Open/Closed**: Las interfaces permiten extender funcionalidad sin modificar cÃ³digo existente
- **Liskov Substitution**: Las implementaciones pueden sustituirse por sus interfaces
- **Interface Segregation**: Interfaces especÃ­ficas y pequeÃ±as (DataRepository, ModelRepository, PreprocessingService)
- **Dependency Inversion**: Las capas superiores dependen de abstracciones, no de implementaciones concretas

## ğŸ“‹ Requisitos

- Python 3.8 o superior
- TensorFlow 2.15+
- NumPy, OpenCV, Pillow
- scikit-learn

## ğŸš€ Inicio RÃ¡pido

### Paso 1: InstalaciÃ³n

1. Crea un entorno virtual (recomendado):
```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

2. Instala las dependencias:
```bash
pip install -r requirements.txt
```

### Paso 2: Entrenar el Modelo (OBLIGATORIO la primera vez)

**âš ï¸ IMPORTANTE:** Debes entrenar el modelo ANTES de usar la cÃ¡mara.

```bash
python3 -m src.interfaces.cli.main train
```

Este proceso puede tardar 10-30 minutos dependiendo de tu hardware.

### Paso 3: Ejecutar la CÃ¡mara

Una vez entrenado el modelo, puedes usar la cÃ¡mara en tiempo real:

```bash
python3 -m src.interfaces.cli.main camera
```

**Â¡Listo!** Ahora puedes hacer seÃ±as frente a la cÃ¡mara y el sistema las detectarÃ¡.

> ğŸ“– Para mÃ¡s detalles, consulta [QUICKSTART.md](QUICKSTART.md)

## ğŸ“Š Datos

El proyecto utiliza el dataset ASL Alphabet que debe estar en `data/asl_alphabet_train/`. El dataset contiene imÃ¡genes organizadas por carpetas, una para cada letra del alfabeto (A-Z) y clases especiales (space, del, nothing).

## ğŸ¯ Uso Detallado

### 1. Entrenar el modelo

```bash
python3 -m src.interfaces.cli.main train
```

Opciones adicionales:
```bash
python -m src.interfaces.cli.main train \
    --data-dir data/asl_alphabet_train \
    --model-path models/asl_model.keras \
    --epochs 20 \
    --batch-size 32
```

### 2. Hacer predicciones desde una imagen

```bash
python3 -m src.interfaces.cli.main predict path/to/image.jpg
```

O especificar un modelo diferente:
```bash
python3 -m src.interfaces.cli.main predict path/to/image.jpg --model-path models/asl_model.keras
```

### 3. ğŸ¥ Modo CÃ¡mara en Tiempo Real

El intÃ©rprete puede activar la cÃ¡mara y detectar letras y palabras en tiempo real:

```bash
python3 -m src.interfaces.cli.main camera
```

**âš ï¸ Requisito:** Debes haber entrenado el modelo primero (Paso 2 del Inicio RÃ¡pido).

**Controles durante la ejecuciÃ³n:**
- `q`: Salir del programa
- `c`: Limpiar la palabra actual
- `r`: Resetear la posiciÃ³n del ROI (regiÃ³n de interÃ©s) al centro

**Opciones adicionales:**
```bash
python -m src.interfaces.cli.main camera \
    --model-path models/asl_model.keras \
    --camera-index 0 \
    --min-confidence 0.7 \
    --stability-threshold 10 \
    --space-delay 2.0
```

**ParÃ¡metros:**
- `--camera-index`: Ãndice de la cÃ¡mara a usar (por defecto: 0)
- `--min-confidence`: Confianza mÃ­nima para aceptar una letra (por defecto: 0.7)
- `--stability-threshold`: NÃºmero de detecciones consecutivas necesarias para agregar una letra (por defecto: 10)
- `--space-delay`: Segundos de detecciÃ³n de 'space' para agregar un espacio (por defecto: 2.0)

**CÃ³mo usar:**
1. Coloca tu mano dentro del rectÃ¡ngulo verde (ROI) en la pantalla
2. Realiza la seÃ±a de la letra que deseas
3. El sistema detectarÃ¡ la letra y la agregarÃ¡ a la palabra actual
4. Para agregar un espacio, mantÃ©n la seÃ±a de 'space' por 2 segundos
5. Para borrar la Ãºltima letra, realiza la seÃ±a de 'del'
6. Las palabras completadas se mostrarÃ¡n en la parte superior de la pantalla

## ğŸ”§ ConfiguraciÃ³n

Puedes modificar los parÃ¡metros de configuraciÃ³n en `config/settings.py`:

- `IMAGE_SIZE`: TamaÃ±o de las imÃ¡genes (por defecto: 64x64)
- `BATCH_SIZE`: TamaÃ±o del batch para entrenamiento (por defecto: 32)
- `EPOCHS`: NÃºmero de Ã©pocas (por defecto: 10)
- `LEARNING_RATE`: Tasa de aprendizaje (por defecto: 0.001)
- `VALIDATION_SPLIT`: Porcentaje de datos para validaciÃ³n (por defecto: 0.2)

## ğŸ“ Estructura del Proyecto

```
sign-language-interpreter/
â”œâ”€â”€ config/              # Configuraciones
â”œâ”€â”€ data/                # Datos de entrenamiento y prueba
â”œâ”€â”€ models/              # Modelos entrenados (generados)
â”œâ”€â”€ src/                 # CÃ³digo fuente
â”‚   â”œâ”€â”€ domain/         # LÃ³gica de negocio
â”‚   â”œâ”€â”€ infrastructure/ # Implementaciones
â”‚   â”œâ”€â”€ application/    # Casos de uso
â”‚   â””â”€â”€ interfaces/     # Interfaces de usuario
â”œâ”€â”€ tests/              # Tests (por implementar)
â”œâ”€â”€ requirements.txt    # Dependencias
â””â”€â”€ README.md          # Este archivo
```

## ğŸ§  Modelo

El modelo utiliza una arquitectura CNN con:
- 4 bloques convolucionales con MaxPooling y BatchNormalization
- Capas de Dropout para regularizaciÃ³n
- Capa densa final con activaciÃ³n softmax para clasificaciÃ³n multiclase

## ğŸ“ Notas

- El entrenamiento puede tardar varios minutos dependiendo del hardware
- Se recomienda usar GPU para acelerar el entrenamiento
- Los modelos entrenados se guardan en el directorio `models/`
- Los checkpoints se guardan automÃ¡ticamente durante el entrenamiento
- Para el modo cÃ¡mara, asegÃºrate de tener buena iluminaciÃ³n y coloca tu mano dentro del rectÃ¡ngulo verde
- El sistema requiere detecciones estables antes de agregar letras a la palabra (configurable con `--stability-threshold`)
- Para formar palabras, realiza las seÃ±as de las letras en secuencia. Usa 'space' para separar palabras y 'del' para borrar

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, asegÃºrate de seguir los principios SOLID y mantener la arquitectura en capas.

## ğŸ“„ Licencia

[Especificar licencia]

